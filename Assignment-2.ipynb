{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b8ef12-2cd5-41c1-82bc-87e185c4bab0",
   "metadata": {},
   "source": [
    "## 1. What are Corpora?\n",
    "**Answer:** Corpora (plural of corpus) are large and structured sets of texts or documents used for statistical analysis and hypothesis testing in linguistic research. They provide a sample of natural language usage and are used to study language patterns and structures.\n",
    "\n",
    "**Example:** The Brown Corpus, the British National Corpus, and the Google Books Corpus are all examples of corpora used in linguistic research and NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What are Tokens?\n",
    "**Answer:** Tokens are individual units of meaning in a text, such as words, punctuation marks, or other symbols. Tokenization is the process of splitting text into these units.\n",
    "\n",
    "**Example:** In the sentence \"ChatGPT is awesome!\", the tokens are [\"ChatGPT\", \"is\", \"awesome\", \"!\"].\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What are Unigrams, Bigrams, Trigrams?\n",
    "**Answer:**\n",
    "- **Unigrams:** Single tokens or words in a text. For instance, in the sentence \"NLP is fun\", the unigrams are [\"NLP\", \"is\", \"fun\"].\n",
    "- **Bigrams:** Sequences of two consecutive tokens. For the same sentence, the bigrams are [\"NLP is\", \"is fun\"].\n",
    "- **Trigrams:** Sequences of three consecutive tokens. In the sentence, the trigrams are [\"NLP is fun\"].\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to generate n-grams from text?\n",
    "**Answer:** N-grams are contiguous sequences of `n` items from a given text. To generate n-grams, you typically follow these steps:\n",
    "1. **Tokenize** the text into a list of tokens.\n",
    "2. **Create n-grams** by taking contiguous sequences of `n` tokens.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from nltk import ngrams\n",
    "\n",
    "text = \"ChatGPT is an amazing tool\"\n",
    "tokens = text.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a6dfe-f107-49ab-88a2-a7114c39dc2f",
   "metadata": {},
   "source": [
    "## 5. Explain Lemmatization\n",
    "**Answer:** Lemmatization is the process of reducing a word to its base or root form. Unlike stemming, it considers the context and converts the word into its meaningful base form.\n",
    "\n",
    "**Example:** The word \"running\" is lemmatized to \"run\", and \"better\" is lemmatized to \"good\".\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_word = lemmatizer.lemmatize(\"running\", pos='v')  # 'run'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e10d4-aa4b-4ee5-8773-e8e7739303ba",
   "metadata": {},
   "source": [
    "## 6. Explain Stemming\n",
    "**Answer:**: Stemming is a process of reducing a word to its root form by removing suffixes. It is less precise than lemmatization and can produce stems that are not actual words.\n",
    "\n",
    "**Example:**: The words \"running\", \"runner\", and \"runs\" might all be stemmed to \"run\".\n",
    "\n",
    "**Example Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a258f1-0c86-4d57-83b2-85ea9d224fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word = stemmer.stem(\"running\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f856c50-c42a-4e7c-aa61-7a1b100efea8",
   "metadata": {},
   "source": [
    "## 7. Explain Part-of-Speech (POS) tagging\n",
    "**Answer:**: POS tagging is the process of identifying the part of speech (noun, verb, adjective, etc.) for each token in a text. It helps in understanding the grammatical structure of the sentence.\n",
    "\n",
    "**Example Code:**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79eea095-98b8-4d7a-84f3-34e243b35a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cla.shehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.word_tokenize(\"ChatGPT is an amazing tool\")\n",
    "pos_tags = nltk.pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23d77e-adb1-4276-9e7c-8bfd352f8d48",
   "metadata": {},
   "source": [
    "## 8. Explain Chunking or Shallow Parsing\n",
    "**Answer:**: Chunking involves grouping tokens into chunks or phrases based on their POS tags. It helps in identifying and extracting meaningful structures from the text, such as noun phrases or verb phrases.\n",
    "\n",
    "**Example Code:**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff89975a-8d12-4bd4-900f-1129d8eff80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = RegexpParser(grammar)\n",
    "chunks = parser.parse(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8495e3c-ce8f-417f-8c09-d10cc4a0902f",
   "metadata": {},
   "source": [
    "## 9. Explain Noun Phrase (NP) chunking\n",
    "**Answer:**: NP Chunking is a specific type of chunking that focuses on identifying and grouping noun phrases within a text. Noun phrases typically consist of a noun and its modifiers.\n",
    "\n",
    "**Example Code:**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339ac41a-a793-4183-96c1-6649a43b926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = RegexpParser(grammar)\n",
    "np_chunks = parser.parse(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d87dc-a35a-473d-bc94-15cc5e803a81",
   "metadata": {},
   "source": [
    "## 10. Explain Named Entity Recognition (NER)\n",
    "**Answer:**: NER is the process of identifying and classifying named entities in text into predefined categories like person names, organizations, locations, dates, etc. It helps in extracting structured information from unstructured text.\n",
    "\n",
    "**Example Code:**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4813a3cc-0475-4a7c-a005-61f69cf5b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bd6e8-97e2-45b8-a90b-eff461b987d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b05cc-18a8-4980-84b1-cdb4d858643b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
