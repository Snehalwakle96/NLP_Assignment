{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355071db-8da0-4edf-b122-8416764a0fd9",
   "metadata": {},
   "source": [
    "### 1. Explain One-Hot Encoding\n",
    "One-Hot Encoding is a technique used to convert categorical variables into a binary matrix representation. In this approach, each unique category value is converted into a vector of zeros and ones. For example, if you have a category with three possible values (`Red`, `Green`, `Blue`), One-Hot Encoding will convert these into three binary vectors:\n",
    "\n",
    "- `Red` -> [1, 0, 0]\n",
    "- `Green` -> [0, 1, 0]\n",
    "- `Blue` -> [0, 0, 1]\n",
    "\n",
    "This method is useful for feeding categorical data into machine learning models, as it allows the model to understand the categories without assuming any ordinal relationship between them.\n",
    "\n",
    "### 2. Explain Bag of Words\n",
    "Bag of Words (BoW) is a simple and commonly used technique for text representation in natural language processing. In this approach, a text document is represented as a collection (or \"bag\") of its words, disregarding grammar and word order but keeping multiplicity. The frequency of each word in the document is counted, and the result is a vector where each element represents the frequency of a particular word in the document. This vector is often large and sparse, especially when dealing with large vocabularies.\n",
    "\n",
    "### 3. Explain Bag of N-Grams\n",
    "Bag of N-Grams extends the Bag of Words model by considering contiguous sequences of `n` words (known as `n-grams`) rather than individual words. For example, for `n=2` (bigrams), the sentence \"I love data science\" would be represented as the bigrams: `[\"I love\", \"love data\", \"data science\"]`. By including n-grams, this model can capture some of the context and word order information that is lost in the simple Bag of Words model.\n",
    "\n",
    "### 4. Explain TF-IDF\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. The TF-IDF value for a word is the product of two terms:\n",
    "\n",
    "- **Term Frequency (TF):** The frequency of the word in the document.\n",
    "- **Inverse Document Frequency (IDF):** A measure of how common or rare the word is across all documents in the corpus. The IDF value is higher for rare words.\n",
    "\n",
    "TF-IDF helps to highlight words that are important to a specific document, reducing the weight of common words that appear in many documents.\n",
    "\n",
    "### 5. What is the OOV problem?\n",
    "The Out-Of-Vocabulary (OOV) problem occurs when a model encounters words during inference that were not present in the training data. Since these words are unseen, the model may struggle to handle them appropriately. OOV issues are particularly problematic in natural language processing tasks like text classification or machine translation. Common solutions include using a special token to represent OOV words or using subword tokenization techniques (e.g., Byte Pair Encoding) that can break down words into smaller, recognizable units.\n",
    "\n",
    "### 6. What are Word Embeddings?\n",
    "Word Embeddings are dense vector representations of words in a continuous vector space where semantically similar words are mapped to nearby points. Unlike one-hot encoding, which is sparse and high-dimensional, word embeddings capture the contextual meaning of words by placing similar words closer together in the vector space. Common methods for creating word embeddings include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "### 7. Explain Continuous Bag of Words (CBOW)\n",
    "Continuous Bag of Words (CBOW) is a neural network-based approach used in the Word2Vec model for learning word embeddings. In CBOW, the model predicts the target word based on its surrounding context words within a window. For example, given the context words \"the cat on the\", the model predicts the target word \"mat\". The CBOW model is efficient and effective at capturing the semantics of words.\n",
    "\n",
    "### 8. Explain SkipGram\n",
    "Skip-Gram is another neural network-based approach used in the Word2Vec model, but unlike CBOW, Skip-Gram predicts the context words given a target word. For example, if the target word is \"mat\", the model predicts the context words \"the\", \"cat\", \"on\", and \"the\". Skip-Gram tends to perform better than CBOW on smaller datasets and for infrequent words because it focuses on predicting multiple context words for each target word.\n",
    "\n",
    "### 9. Explain GloVe Embeddings\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding technique that combines the benefits of matrix factorization (like Latent Semantic Analysis) and local context-based methods (like Word2Vec). GloVe constructs a co-occurrence matrix from a corpus, capturing how often words appear together. The model then factorizes this matrix to create word vectors. GloVe embeddings represent the global statistical information of the corpus, making them effective at capturing both the local context of words and the overall global structure of the language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fef28-c024-4ca9-876b-ef9d262756d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
