{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e8768f-9b8c-4436-940a-8716966eba90",
   "metadata": {},
   "source": [
    "## 1. What are Vanilla Autoencoders?\n",
    "**Answer:** Vanilla autoencoders are a type of neural network used for unsupervised learning. They consist of two parts: an encoder, which compresses the input data into a lower-dimensional representation (latent space), and a decoder, which reconstructs the original data from this representation. The goal is to minimize the difference between the input and the reconstructed output.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What are Sparse Autoencoders?\n",
    "**Answer:** Sparse autoencoders are a variant of vanilla autoencoders that impose a sparsity constraint on the hidden units during training. This constraint encourages the model to learn a more efficient representation by activating only a few neurons in the hidden layer, which can lead to more meaningful features.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Sparsity Penalty:** Often implemented using an L1 regularization term or KL divergence to enforce sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What are Denoising Autoencoders?\n",
    "**Answer:** Denoising autoencoders are designed to reconstruct the original input from a corrupted version of it. During training, noise is added to the input data, and the model learns to remove this noise, thereby improving the robustness of the learned representations.\n",
    "\n",
    "**Purpose:**\n",
    "- To make the model more robust to noise and improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What are Convolutional Autoencoders?\n",
    "**Answer:** Convolutional autoencoders use convolutional layers instead of fully connected layers in both the encoder and decoder parts. They are particularly well-suited for image data, as they can capture spatial hierarchies and reduce the number of parameters.\n",
    "\n",
    "**Application:**\n",
    "- Image denoising, image compression, and feature extraction from images.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What are Stacked Autoencoders?\n",
    "**Answer:** Stacked autoencoders are deep neural networks formed by stacking multiple layers of autoencoders. Each layer is trained to encode the representation from the previous layer, allowing the network to learn more complex and hierarchical features.\n",
    "\n",
    "**Training Method:**\n",
    "- Often trained layer by layer in a greedy fashion to ensure proper initialization before fine-tuning the entire network.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Explain How to Generate Sentences Using LSTM Autoencoders\n",
    "**Answer:** LSTM autoencoders can be used to generate sentences by first encoding an input sentence into a fixed-length vector using an LSTM encoder and then decoding this vector into a sequence of words using an LSTM decoder. During generation, the decoder can be conditioned on previously generated words to create coherent sentences.\n",
    "\n",
    "**Steps:**\n",
    "1. **Encoding:** Convert the input sentence into a latent representation.\n",
    "2. **Decoding:** Use the latent representation to generate words one by one until an end-of-sentence token is produced.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Explain Extractive Summarization\n",
    "**Answer:** Extractive summarization is a technique that involves selecting and extracting key sentences or phrases directly from the source text to create a summary. The goal is to retain the most important information from the original content without generating new text.\n",
    "\n",
    "**Techniques:**\n",
    "- **Sentence Ranking:** Rank sentences based on their importance.\n",
    "- **TextRank Algorithm:** A graph-based ranking algorithm for extracting key sentences.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Explain Abstractive Summarization\n",
    "**Answer:** Abstractive summarization involves generating new sentences that capture the essence of the source text, rather than just selecting existing sentences. This approach often requires a deeper understanding of the content and the ability to paraphrase and generalize information.\n",
    "\n",
    "**Techniques:**\n",
    "- **Sequence-to-Sequence Models:** Use encoder-decoder architectures to generate summaries.\n",
    "- **Transformer Models:** Leverage self-attention mechanisms for more coherent summaries.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Explain Beam Search\n",
    "**Answer:** Beam search is a heuristic search algorithm used in sequence generation tasks like machine translation and text summarization. It explores multiple possible sequences at each time step and keeps the top-k sequences (beams) based on their cumulative probabilities.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Beam Width:** The number of sequences retained at each step.\n",
    "- **Pruning:** Discarding lower-probability sequences to focus on more promising candidates.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Explain Length Normalization\n",
    "**Answer:** Length normalization is a technique used in sequence generation to adjust the probabilities of sequences based on their length. This prevents the model from favoring shorter sequences that might have higher raw probabilities due to the multiplication of probabilities for each word.\n",
    "\n",
    "**Equation:**\n",
    "\\[ \\text{Normalized Score} = \\frac{1}{L^\\alpha} \\cdot \\text{Score} \\]\n",
    "where \\( L \\) is the length of the sequence and \\( \\alpha \\) is a hyperparameter.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Explain Coverage Normalization\n",
    "**Answer:** Coverage normalization is used in sequence generation to ensure that the generated sequence covers all relevant parts of the input. It addresses the issue of over- or under-generation of certain parts by incorporating a coverage vector that tracks how much of the input has been attended to.\n",
    "\n",
    "**Application:**\n",
    "- Reduces redundancy and repetition in generated text.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Explain ROUGE Metric Evaluation\n",
    "**Answer:** ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of machine-generated text by comparing it to reference summaries. It measures the overlap of n-grams, word sequences, and word pairs between the generated and reference texts.\n",
    "\n",
    "**Common Variants:**\n",
    "- **ROUGE-N:** Measures n-gram overlap.\n",
    "- **ROUGE-L:** Measures the longest common subsequence (LCS) between texts.\n",
    "- **ROUGE-S:** Measures the overlap of skip-bigrams.\n",
    "\n",
    "**Usage:**\n",
    "- Commonly used in evaluating the performance of summarization and translation systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb3e8b-5ff7-4855-83c9-55471cef69a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
