{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5df98f4-b655-469f-a5f0-9582c40a6f4a",
   "metadata": {},
   "source": [
    "## 1. Explain the architecture of BERT\n",
    "**Answer:** BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language understanding tasks. The architecture consists of multiple layers of bidirectional transformer encoders. BERT reads the entire sequence of words simultaneously, allowing it to understand context from both directions (left-to-right and right-to-left).\n",
    "\n",
    "**Key Components:**\n",
    "- **Input Embeddings:** Token embeddings, segment embeddings, and position embeddings.\n",
    "- **Encoder Layers:** Stacked layers of bidirectional transformers.\n",
    "- **Output:** Contextualized word embeddings used for various NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explain Masked Language Modeling (MLM)\n",
    "**Answer:** Masked Language Modeling (MLM) is a pretraining task used in BERT. In MLM, some percentage of the input tokens are randomly masked, and the model is trained to predict the original tokens based on the context provided by the other tokens. This encourages the model to learn bidirectional context and improves its language understanding.\n",
    "\n",
    "**Process:**\n",
    "- **Masking:** Randomly mask 15% of the tokens in the input.\n",
    "- **Prediction:** Predict the original token for each masked position.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Explain Next Sentence Prediction (NSP)\n",
    "**Answer:** Next Sentence Prediction (NSP) is another pretraining task used in BERT. The model is trained to predict whether a given pair of sentences (A and B) are consecutive sentences in the original text. This helps BERT understand the relationship between sentences, which is crucial for tasks like question answering and sentence pairs classification.\n",
    "\n",
    "**Process:**\n",
    "- **Training Pairs:** For 50% of the time, B is the actual next sentence after A; for the other 50%, a random sentence is used as B.\n",
    "- **Objective:** Classify whether B is the actual next sentence of A.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What is Matthews evaluation?\n",
    "**Answer:** Matthews evaluation typically refers to the Matthews Correlation Coefficient (MCC) evaluation metric. It is used to measure the quality of binary (two-class) classifications. It takes into account true positives, true negatives, false positives, and false negatives, providing a balanced measure that can be used even with imbalanced classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What is Matthews Correlation Coefficient (MCC)?\n",
    "**Answer:** The Matthews Correlation Coefficient (MCC) is a metric used for evaluating binary classification tasks. It is a measure of the strength of the relationship between the observed and predicted classifications. MCC returns a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates complete disagreement between prediction and observation.\n",
    "\n",
    "**Equation:**\n",
    "\\[ \\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} \\]\n",
    "where TP, TN, FP, FN are true positives, true negatives, false positives, and false negatives, respectively.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Explain Semantic Role Labeling\n",
    "**Answer:** Semantic Role Labeling (SRL) is the process of identifying the predicate-argument structure in a sentence. It involves determining \"who\" did \"what\" to \"whom\", \"when\", \"where\", and \"how\". SRL assigns labels to words or phrases in a sentence to indicate their semantic roles (e.g., agent, patient, instrument).\n",
    "\n",
    "**Example:**\n",
    "- Sentence: \"John (agent) gave (predicate) a book (theme) to Mary (recipient).\"\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "**Answer:** Fine-tuning a BERT model takes less time than pretraining because pretraining involves learning general language representations from scratch, which requires processing a vast amount of text data and adjusting millions of parameters over many epochs. Fine-tuning, on the other hand, is done on a specific task with a smaller dataset and fewer epochs, where the model already has learned general language features during pretraining and only needs to adjust for the particular task.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Recognizing Textual Entailment (RTE)\n",
    "**Answer:** Recognizing Textual Entailment (RTE) is a natural language processing task where the goal is to determine whether a given hypothesis is entailed by a given premise. In other words, it checks if the truth of one text fragment (the hypothesis) logically follows from another text fragment (the premise).\n",
    "\n",
    "**Classification:**\n",
    "- **Entailment:** The hypothesis is true given the premise.\n",
    "- **Contradiction:** The hypothesis is false given the premise.\n",
    "- **Neutral:** The hypothesis is neither true nor false given the premise.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Explain the decoder stack of GPT models\n",
    "**Answer:** The decoder stack of GPT (Generative Pre-trained Transformer) models consists of a series of transformer decoder blocks. Unlike the BERT model, GPT is a unidirectional model, meaning it processes the input tokens in a left-to-right manner. Each decoder block has the following components:\n",
    "\n",
    "**Components:**\n",
    "- **Masked Multi-Head Self-Attention:** Prevents attending to future tokens by masking future positions.\n",
    "- **Feedforward Neural Network:** A fully connected feedforward network applied to the output of the attention mechanism.\n",
    "- **Layer Normalization:** Applied after each attention and feedforward network to stabilize training.\n",
    "- **Residual Connections:** Skip connections that help in gradient flow and prevent vanishing gradients.\n",
    "\n",
    "**Purpose:** GPT models are designed for text generation tasks and can generate coherent text by predicting the next word in a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c78c9-d392-45de-ab40-6599dbc20d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
