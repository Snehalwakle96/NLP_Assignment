{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34af769f-aade-4c55-833f-185d8f1dafa8",
   "metadata": {},
   "source": [
    "## 1. Explain the basic architecture of RNN cell\n",
    "**Answer:** The basic architecture of an RNN cell involves a loop that allows information to persist. The core idea is to maintain a hidden state that is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "- **Hidden State (h_t):** Contains information from previous time steps.\n",
    "- **Input (x_t):** The current input to the RNN cell.\n",
    "- **Weight Matrices (W_hh, W_xh, W_hy):** Used to compute the new hidden state and output.\n",
    "- **Activation Function:** Typically, the `tanh` or `ReLU` function is used to introduce non-linearity.\n",
    "\n",
    "**Basic Equation:** \n",
    "\\[ h_t = \\text{activation}(W_{hh} h_{t-1} + W_{xh} x_t + b) \\]\n",
    "\\[ y_t = W_{hy} h_t + b \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explain Backpropagation through time (BPTT)\n",
    "**Answer:** Backpropagation through time (BPTT) is an extension of backpropagation for training RNNs. It involves unfolding the RNN through time, treating it as a deep feedforward network, and then applying the standard backpropagation algorithm.\n",
    "\n",
    "- **Unfolding:** Expand the RNN across all time steps.\n",
    "- **Error Calculation:** Compute the gradient of the loss function with respect to each weight by summing gradients across all time steps.\n",
    "- **Weight Update:** Update the weights based on the computed gradients.\n",
    "\n",
    "**Steps:**\n",
    "1. Forward pass through the unfolded RNN to compute outputs and loss.\n",
    "2. Backward pass through the unfolded RNN to compute gradients and update weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Explain Vanishing and exploding gradients\n",
    "**Answer:**\n",
    "- **Vanishing Gradients:** During backpropagation, gradients can become very small, causing the network weights to stop changing significantly. This leads to slow or stalled training, especially in long sequences.\n",
    "- **Exploding Gradients:** Gradients can grow exponentially large during backpropagation, causing unstable training and leading to weight values that are too large.\n",
    "\n",
    "**Solutions:**\n",
    "- **Vanishing Gradients:** Use activation functions like `ReLU` or advanced architectures like LSTMs.\n",
    "- **Exploding Gradients:** Use gradient clipping to limit the size of gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Explain Long Short-Term Memory (LSTM)\n",
    "**Answer:** LSTM is a type of RNN designed to address the vanishing gradient problem. It uses a special architecture with gating mechanisms to control the flow of information and maintain long-term dependencies.\n",
    "\n",
    "**Key Components:**\n",
    "- **Forget Gate (f_t):** Decides what information to discard from the cell state.\n",
    "- **Input Gate (i_t):** Controls the amount of new information to add to the cell state.\n",
    "- **Cell State (C_t):** Stores the long-term memory.\n",
    "- **Output Gate (o_t):** Determines the output based on the cell state.\n",
    "\n",
    "**Basic Equations:**\n",
    "```python\n",
    "f_t = σ(W_f [h_{t-1}, x_t] + b_f)\n",
    "i_t = σ(W_i [h_{t-1}, x_t] + b_i)\n",
    "C_t' = tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "C_t = f_t * C_{t-1} + i_t * C_t'\n",
    "o_t = σ(W_o [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t * tanh(C_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782ae90-056e-46ff-8544-8c7f3bd3ec1a",
   "metadata": {},
   "source": [
    "## 5. Explain Gated Recurrent Unit (GRU)\n",
    "**Answer:** GRU is a variant of LSTM with a simpler architecture. It combines the forget and input gates into a single update gate and does not use a separate cell state. \n",
    "\n",
    "**Key Components:**\n",
    "- **Update Gate (z_t):** Determines how much of the past information to keep.\n",
    "- **Reset Gate (r_t):** Controls how much of the past information to forget.\n",
    "- **New Memory Content (h_t'):** Computes new information to be added to the current state.\n",
    "\n",
    "**Basic Equations:**\n",
    "```python\n",
    "z_t = σ(W_z [h_{t-1}, x_t] + b_z)\n",
    "r_t = σ(W_r [h_{t-1}, x_t] + b_r)\n",
    "h_t' = tanh(W_h [r_t * h_{t-1}, x_t] + b_h)\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * h_t'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aab043-0045-4fb5-b61b-c0a95fc0fa92",
   "metadata": {},
   "source": [
    "## 6. Explain Peephole LSTM\n",
    "**Answer:**: Peephole LSTM is an enhancement of the standard LSTM architecture where the gates are allowed to access the cell state directly. This can improve the network’s ability to model dependencies by providing additional information to the gates.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "**Peephole Connections**: Allow the gates to use the cell state directly in their computations.\n",
    "\n",
    "**Basic Equations**:\n",
    "```python\n",
    "\n",
    "f_t = σ(W_f [h_{t-1}, x_t] + U_f C_{t-1} + b_f)\n",
    "i_t = σ(W_i [h_{t-1}, x_t] + U_i C_{t-1} + b_i)\n",
    "C_t' = tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "C_t = f_t * C_{t-1} + i_t * C_t'\n",
    "o_t = σ(W_o [h_{t-1}, x_t] + U_o C_t + b_o)\n",
    "h_t = o_t * tanh(C_t)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf8e81-3ba3-41b4-b566-2139a158db25",
   "metadata": {},
   "source": [
    "## 7. Bidirectional RNNs\n",
    "**Answer:** Bidirectional RNNs (BiRNNs) process sequences in both forward and backward directions. This allows the network to access both past and future context for each time step.\n",
    "\n",
    "**Architecture:**\n",
    "- **Forward RNN:** Processes the sequence from start to end.\n",
    "- **Backward RNN:** Processes the sequence from end to start.\n",
    "\n",
    "**Output:** The final output is typically obtained by concatenating or combining the outputs from both the forward and backward RNN layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Explain the gates of LSTM with equations\n",
    "**Answer:** The gates in an LSTM cell control the flow of information and help manage long-term dependencies.\n",
    "\n",
    "- **Forget Gate:** Determines what information to discard from the cell state.\n",
    "  \\[ f_t = σ(W_f [h_{t-1}, x_t] + b_f) \\]\n",
    "- **Input Gate:** Decides which values to update in the cell state.\n",
    "  \\[ i_t = σ(W_i [h_{t-1}, x_t] + b_i) \\]\n",
    "- **Cell State Update:** Computes new information to be added to the cell state.\n",
    "  \\[ C_t' = tanh(W_C [h_{t-1}, x_t] + b_C) \\]\n",
    "- **Cell State:** Updates the cell state with the new information.\n",
    "  \\[ C_t = f_t * C_{t-1} + i_t * C_t' \\]\n",
    "- **Output Gate:** Controls the output based on the updated cell state.\n",
    "  \\[ o_t = σ(W_o [h_{t-1}, x_t] + b_o) \\]\n",
    "  \\[ h_t = o_t * tanh(C_t) \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Explain BiLSTM\n",
    "**Answer:** BiLSTM (Bidirectional LSTM) is an extension of LSTM that processes sequences in both forward and backward directions. This helps capture context from both past and future, improving the model's performance on various tasks.\n",
    "\n",
    "**Architecture:**\n",
    "- **Forward LSTM:** Processes the sequence from the start to the end.\n",
    "- **Backward LSTM:** Processes the sequence from the end to the start.\n",
    "\n",
    "**Output:** The final output is typically obtained by concatenating or combining the outputs from both the forward and backward LSTM layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Explain BiGRU\n",
    "**Answer:** BiGRU (Bidirectional GRU) is a variant of GRU that processes sequences in both forward and backward directions. It combines the advantages of GRUs with bidirectional context.\n",
    "\n",
    "**Architecture:**\n",
    "- **Forward GRU:** Processes input from the beginning to the end.\n",
    "- **Backward GRU:** Processes input from the end to the beginning.\n",
    "\n",
    "**Output:** The final output is typically a concatenation or combination of the outputs from both the forward and backward GRU layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9310afe-7cd9-4c44-a114-b2fff10d733d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
