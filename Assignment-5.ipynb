{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffec2be-292f-4fcf-b01f-161fecf3fd52",
   "metadata": {},
   "source": [
    "## 1. What are Sequence-to-Sequence Models?\n",
    "**Answer:** Sequence-to-sequence (seq2seq) models are a type of neural network architecture designed to convert sequences from one domain into sequences in another domain. They are typically used for tasks where both the input and output are sequences of variable lengths.\n",
    "\n",
    "**Components:**\n",
    "- **Encoder:** Processes the input sequence and converts it into a context vector (a fixed-size representation).\n",
    "- **Decoder:** Takes the context vector and generates the output sequence.\n",
    "\n",
    "**Applications:**\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Speech Recognition\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What are the Problems with Vanilla RNNs?\n",
    "**Answer:** Vanilla RNNs face several issues:\n",
    "- **Vanishing Gradient Problem:** Gradients can become very small during backpropagation, making it difficult for the network to learn long-term dependencies.\n",
    "- **Exploding Gradient Problem:** Gradients can become excessively large, leading to unstable training.\n",
    "- **Difficulty in Capturing Long-Term Dependencies:** Vanilla RNNs struggle with sequences where dependencies span many time steps.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What is Gradient Clipping?\n",
    "**Answer:** Gradient clipping is a technique used to address the exploding gradient problem in neural networks. It involves setting a threshold value and scaling down gradients that exceed this threshold. This prevents gradients from becoming too large and destabilizing the training process.\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(clipnorm=1.0)  # Clip gradients by norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42552f03-0b86-492d-bd83-affb7300b7f4",
   "metadata": {},
   "source": [
    "## 4. Explain the Attention Mechanism\n",
    "**Answer:** The attention mechanism allows models to focus on different parts of the input sequence when generating each part of the output sequence. It enhances the modelâ€™s ability to handle long-range dependencies by weighing the importance of different parts of the input.\n",
    "\n",
    "**Key Components:**\n",
    "- **Alignment Scores:** Measure how well each part of the input matches the current part of the output.\n",
    "- **Context Vector:** A weighted sum of the input features based on the alignment scores.\n",
    "\n",
    "**Equation:**\n",
    "\\[ \\text{Context} = \\sum (\\text{Alignment Score} \\times \\text{Input}) \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Explain Conditional Random Fields (CRFs)\n",
    "**Answer:** Conditional Random Fields (CRFs) are a type of probabilistic graphical model used for predicting sequences. They model the conditional probability of a sequence of labels given a sequence of observations, capturing dependencies between labels.\n",
    "\n",
    "**Key Features:**\n",
    "- **Structured Prediction:** CRFs consider the entire sequence for prediction rather than individual labels.\n",
    "- **Feature Functions:** CRFs can incorporate various features from the data to improve predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Explain Self-Attention\n",
    "**Answer:** Self-attention is a mechanism where each element in a sequence attends to all other elements in the same sequence to produce a representation. It helps capture relationships between elements irrespective of their positions.\n",
    "\n",
    "**Key Components:**\n",
    "- **Query, Key, and Value Vectors:** Used to compute attention scores and weighted sums.\n",
    "- **Attention Scores:** Measure the relevance of each element to others.\n",
    "\n",
    "**Equation:**\n",
    "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56e272-1e78-43c6-b214-c320e304c972",
   "metadata": {},
   "source": [
    "## 7. What is Bahdanau Attention?\r\n",
    "**Answer:** Bahdanau Attention, also known as additive attention, is a type of attention mechanism that computes alignment scores using a feedforward neural network. It helps improve the performance of sequence-to-sequence models by allowing them to focus on different parts of the input sequence.\r\n",
    "\r\n",
    "**Components:**\r\n",
    "- **Alignment Scores:** Computed using a neural network.\r\n",
    "- **Context Vector:** Weighted sum of the input sequence based on the alignment scores.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ \\text{Score}(h_t, s_{t-1}) = v^T \\text{tanh}(W_h h_t + W_s s_{t-1} + b) \\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 8. What is a Language Model?\r\n",
    "**Answer:** A language model is a statistical model that predicts the probability of a sequence of words. It captures the likelihood of a word given its preceding context and is used in various NLP tasks.\r\n",
    "\r\n",
    "**Types:**\r\n",
    "- **Unigram Model:** Considers individual words.\r\n",
    "- **N-gram Model:** Considers sequences of n words.\r\n",
    "- **Neural Language Models:** Use neural networks to capture complex patterns.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 9. What is Multi-Head Attention?\r\n",
    "**Answer:** Multi-head attention is an extension of the attention mechanism that uses multiple attention heads to capture different aspects of the input sequence. Each head learns to focus on different parts of the sequence, and their outputs are combined to produce the final representation.\r\n",
    "\r\n",
    "**Key Components:**\r\n",
    "- **Multiple Attention Heads:** Each head learns different attention patterns.\r\n",
    "- **Concatenation and Linear Transformation:** Combine the outputs of all heads.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h) W^O \\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 10. What is Bilingual Evaluation Understudy (BLEU)?\r\n",
    "**Answer:** BLEU (Bilingual Evaluation Understudy) is an evaluation metric for machine translation and text generation. It measures the quality of translated text by comparing it to one or more reference translations.\r\n",
    "\r\n",
    "**Key Features:**\r\n",
    "- **N-gram Precision:** Measures the overlap of n-grams between the generated and reference texts.\r\n",
    "- **Brevity Penalty:** Penalizes overly short translations.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ \\text{BLEU} = \\text{BP} \\cdot \\exp \\left( \\sum_{n=1}^N p_n \\right) \\]\r\n",
    "where BP is the brevity penalty and \\( p_n \\) is the precision for n-grams.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0df57-d466-4484-a884-6998354c22a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
